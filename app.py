import streamlit as st
import requests
from bs4 import BeautifulSoup
import openai
import pandas as pd
from docx import Document
import json

# ----------------- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© -----------------
st.set_page_config(page_title="Dream SEO Enhancer", page_icon="ğŸŒ™", layout="wide")
openai.api_key = st.secrets["OPENAI_API_KEY"]

# ----------------- Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© -----------------
def fetch_competitor_text(url):
    """Ø¬Ù„Ø¨ Ù†Øµ Ù…Ù† Ø±Ø§Ø¨Ø· Ù…Ù†Ø§ÙØ³"""
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        return "\n".join(paragraphs[:40])
    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù†Øµ: {e}"

def analyze_article(article, competitors, keyword, related_keywords):
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"""
    competitor_texts = "\n\n".join([f"[Ø§Ù„Ù…Ù†Ø§ÙØ³ {i+1}]\n{txt}" for i, txt in enumerate(competitors)])
    prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± SEO ÙÙŠ Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù….
Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:
1- Ø­Ù„Ù‘Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ§ÙƒØ´Ù Ù…Ø´Ø§ÙƒÙ„Ù‡ Ø­Ø³Ø¨ Google Helpful Content Ùˆ E-E-A-T.
2- Ø­Ù„Ù‘Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø£Ø®ÙˆØ°Ø© Ù…Ù† Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†.
3- Ø§Ø³ØªØ®Ø±Ø¬ Ù„Ù…Ø§Ø°Ø§ ÙŠØªØµØ¯Ø±ÙˆÙ† ÙÙŠ Ø¬ÙˆØ¬Ù„ (Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©).
4- Ø§Ø³ØªØ®Ø±Ø¬ Ø£Ù‡Ù… Ù…Ø´Ø§ÙƒÙ„Ù‡Ù….
5- Ø§Ù‚ØªØ±Ø­ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ.
6- Ø§ÙØ­Øµ ÙƒØ«Ø§ÙØ© Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© "{keyword}" ÙˆÙ‡Ù„ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­.
7- Ø±Ø§Ù‚Ø¨ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©: {", ".join(related_keywords)}.

--- Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ---
{article}

--- Ø§Ù„Ù…Ù†Ø§ÙØ³ÙˆÙ† ---
{competitor_texts}
"""
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
    )
    return response.choices[0].message.content

def rewrite_article(article, competitors, keyword, related_keywords, length_choice):
    """Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„"""
    competitor_texts = "\n\n".join([f"[Ø§Ù„Ù…Ù†Ø§ÙØ³ {i+1}]\n{txt}" for i, txt in enumerate(competitors)])

    length_map = {
        "Ù‚ØµÙŠØ± (700-900 ÙƒÙ„Ù…Ø©)": "Ø­ÙˆØ§Ù„ÙŠ 800 ÙƒÙ„Ù…Ø©",
        "Ù…ØªÙˆØ³Ø· (1000-1300 ÙƒÙ„Ù…Ø©)": "Ø­ÙˆØ§Ù„ÙŠ 1200 ÙƒÙ„Ù…Ø©",
        "Ø·ÙˆÙŠÙ„ (1500-2000 ÙƒÙ„Ù…Ø©)": "Ø­ÙˆØ§Ù„ÙŠ 1700 ÙƒÙ„Ù…Ø©"
    }

    prompt = f"""
Ø£Ø¹Ø¯ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¹Ù† ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù… Ø¨Ø­ÙŠØ«:
- ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: "{keyword}" 3-5 Ù…Ø±Ø§Øª Ø¨Ø´ÙƒÙ„ Ø·Ø¨ÙŠØ¹ÙŠ.
- ÙŠØ¯Ù…Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©: {", ".join(related_keywords)} ÙÙŠ Ø§Ù„Ù†Øµ Ø¨Ø´ÙƒÙ„ Ù…Ù†Ø·Ù‚ÙŠ.
- ÙŠÙƒÙˆÙ† Ø§Ù„Ø·ÙˆÙ„ {length_map[length_choice]}.
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø®Ø¨Ø±Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…Ù†Ù‡Ø¬ÙŠØ© ÙˆØ§Ø¶Ø­Ø©.
- ÙŠØ¶ÙŠÙ Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆØ­Ø³Ù‘ÙŠØ©.
- ÙŠÙ‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ù…Ø¯Ø§Ø±Ø³ Ø§Ù„ØªÙØ³ÙŠØ± (Ø§Ø¨Ù† Ø³ÙŠØ±ÙŠÙ†ØŒ Ø§Ù„Ù†Ø§Ø¨Ù„Ø³ÙŠØŒ Ø§Ø¨Ù† Ø´Ø§Ù‡ÙŠÙ†...).
- ÙŠØ¨Ø±Ø² Ø§Ù„Ø´ÙØ§ÙÙŠØ© (Ù‡Ø°Ø§ Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù„Ø§ Ø­ØªÙ…ÙŠ).
- ÙŠÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ù‚Ø§Ø±Ø¦.
- ÙŠØªÙØ§Ø¯Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ø­Ø´Ùˆ.
- Ù…Ù†Ø¸Ù… Ø¨Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© ÙˆØ§Ø¶Ø­Ø©.
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙÙ‚Ø±Ø© Ø®ØªØ§Ù…ÙŠØ© ØªØ­ÙÙŠØ²ÙŠØ©.
- ÙŠÙ‚ØªØ±Ø­ Ù‚Ø³Ù… FAQ ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©.
- ÙŠÙ‚ØªØ±Ø­ Ù‚Ø³Ù… "ğŸ“š Ø§Ù„Ù…ØµØ§Ø¯Ø±" ÙÙŠ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©.

--- Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ---
{article}

--- Ø§Ù„Ù…Ù†Ø§ÙØ³ÙˆÙ† (Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù†Ù‚Ø§Ø· Ù‚ÙˆØªÙ‡Ù…) ---
{competitor_texts}
"""
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.6,
    )
    return response.choices[0].message.content

def apply_internal_links(article, links_df):
    """Ø¥Ø¶Ø§ÙØ© Ø±ÙˆØ§Ø¨Ø· Ø¯Ø§Ø®Ù„ÙŠØ© Ø¹Ù„Ù‰ Ø¬Ù…Ù„ Ø£Ùˆ Ø¹Ø¨Ø§Ø±Ø§Øª"""
    new_article = article
    for _, row in links_df.iterrows():
        phrase, link = row["Ø§Ù„Ù†Øµ"], row["Ø§Ù„Ø±Ø§Ø¨Ø·"]
        if phrase and link:
            new_article = new_article.replace(phrase, f"[{phrase}]({link})", 1)
    return new_article

def export_docx(text, filename="article.docx"):
    doc = Document()
    for line in text.split("\n"):
        doc.add_paragraph(line)
    doc.save(filename)
    return filename

# ----------------- ÙˆØ§Ø¬Ù‡Ø© Streamlit -----------------
st.title("ğŸŒ™ Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù… (SEO + E-E-A-T)")

with st.expander("ğŸ“¥ Ø¥Ø¯Ø®Ø§Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„"):
    article_input = st.text_area("Ø§Ù„ØµÙ‚ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§:", height=300)
    keyword = st.text_input("ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©")
    related_keywords = st.text_area("ğŸ“Œ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© (Ø§ÙØµÙ„ Ø¨ÙŠÙ†Ù‡Ø§ Ø¨ÙØ§ØµÙ„Ø©)")
    related_list = [k.strip() for k in related_keywords.split(",") if k.strip()]

    length_choice = st.selectbox(
        "ğŸ“ Ø§Ø®ØªØ± Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨",
        ["Ù‚ØµÙŠØ± (700-900 ÙƒÙ„Ù…Ø©)", "Ù…ØªÙˆØ³Ø· (1000-1300 ÙƒÙ„Ù…Ø©)", "Ø·ÙˆÙŠÙ„ (1500-2000 ÙƒÙ„Ù…Ø©)"]
    )

    col1, col2, col3 = st.columns(3)
    with col1:
        comp1 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 1")
    with col2:
        comp2 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 2")
    with col3:
        comp3 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 3")

competitors_texts = []
if comp1: competitors_texts.append(fetch_competitor_text(comp1))
if comp2: competitors_texts.append(fetch_competitor_text(comp2))
if comp3: competitors_texts.append(fetch_competitor_text(comp3))

# ---- Ø§Ù„ØªØ­Ù„ÙŠÙ„ ----
if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"):
    if article_input and keyword:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
            report = analyze_article(article_input, competitors_texts, keyword, related_list)
        st.subheader("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        st.write(report)
    else:
        st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©.")

# ---- Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø© ----
if st.button("âœï¸ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„"):
    if article_input and keyword:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©..."):
            rewritten = rewrite_article(article_input, competitors_texts, keyword, related_list, length_choice)
        st.subheader("ğŸ“ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†")
        st.session_state["rewritten_article"] = rewritten
        st.write(rewritten)

        # Ø¹Ø¯Ø§Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
        word_count = len(rewritten.split())
        st.info(f"Ø¹Ø¯Ø¯ ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„: {word_count}")
    else:
        st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©.")

# ---- Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© ----
if "rewritten_article" in st.session_state:
    st.subheader("ğŸ”— Ø£Ø¶Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Anchors)")
    links_df = st.data_editor(
        pd.DataFrame([{"Ø§Ù„Ù†Øµ": "", "Ø§Ù„Ø±Ø§Ø¨Ø·": ""}]),
        num_rows="dynamic",
        use_container_width=True
    )
    if st.button("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©"):
        updated_article = apply_internal_links(st.session_state["rewritten_article"], links_df)
        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©")
        st.write(updated_article)
        st.session_state["final_article"] = updated_article

# ---- Ø§Ù„ØªØµØ¯ÙŠØ± ----
if "final_article" in st.session_state:
    st.subheader("ğŸ’¾ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„")
    final_text = st.session_state["final_article"]

    st.download_button("ØªØ­Ù…ÙŠÙ„ DOCX", data=final_text.encode("utf-8"), file_name="article.docx")
    st.download_button("ØªØ­Ù…ÙŠÙ„ Markdown", data=final_text.encode("utf-8"), file_name="article.md")
    json_data = json.dumps({"article": final_text}, ensure_ascii=False, indent=2)
    st.download_button("ØªØ­Ù…ÙŠÙ„ JSON", data=json_data.encode("utf-8"), file_name="article.json")
