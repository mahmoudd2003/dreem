import streamlit as st
import requests
from bs4 import BeautifulSoup
import openai
import pandas as pd
from docx import Document
import json

# --------------- Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© -----------------
st.set_page_config(page_title="Dream Article Enhancer", page_icon="ğŸŒ™", layout="wide")
openai.api_key = st.secrets["OPENAI_API_KEY"]

# --------------- Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© -----------------
def fetch_competitor_text(url):
    """Ø¬Ù„Ø¨ Ù†Øµ Ù…Ù† Ø±Ø§Ø¨Ø· Ù…Ù†Ø§ÙØ³"""
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        return "\n".join(paragraphs[:40])
    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù†Øµ: {e}"

def analyze_article(article, competitors):
    competitor_texts = "\n\n".join([f"[Ø§Ù„Ù…Ù†Ø§ÙØ³ {i+1}]\n{txt}" for i, txt in enumerate(competitors)])
    prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ SEO Ùˆ E-E-A-T Ù„Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù….
Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: 
1- Ø­Ù„Ù‘Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙˆØ§ÙƒØ´Ù Ù…Ø´Ø§ÙƒÙ„Ù‡ Ø­Ø³Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± Google Helpful Content ÙˆE-E-A-T.
2- Ø­Ù„Ù‘Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø£Ø®ÙˆØ°Ø© Ù…Ù† Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ† Ø§Ù„Ø«Ù„Ø§Ø«Ø©.
3- Ø§Ø³ØªØ®Ø±Ø¬ Ù„Ù…Ø§Ø°Ø§ ÙŠØªØµØ¯Ø±ÙˆÙ† ÙÙŠ Ø¬ÙˆØ¬Ù„ (Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ©).
4- Ø§Ø³ØªØ®Ø±Ø¬ Ø£Ù‡Ù… Ù…Ø´Ø§ÙƒÙ„Ù‡Ù….
5- Ø§Ù‚ØªØ±Ø­ ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ù„ÙŠØªÙÙˆÙ‚ Ø¹Ù„ÙŠÙ‡Ù….

--- Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ---
{article}

--- Ø§Ù„Ù…Ù†Ø§ÙØ³ÙˆÙ† ---
{competitor_texts}
"""
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4,
    )
    return response.choices[0].message.content

def rewrite_article(article, competitors):
    competitor_texts = "\n\n".join([f"[Ø§Ù„Ù…Ù†Ø§ÙØ³ {i+1}]\n{txt}" for i, txt in enumerate(competitors)])
    prompt = f"""
Ø£Ø¹Ø¯ ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¹Ù† ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù… Ù„ÙŠÙƒÙˆÙ†:
- Ø¨Ø¹ÙŠØ¯ Ø¹Ù† Ø§Ù„Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ù‚Ø§Ù„Ø¨ÙŠØ© ÙˆØ§Ù„Ø§ÙØªØªØ§Ø­ÙŠØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©.
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø®Ø¨Ø±Ø© Ù…Ø¨Ø§Ø´Ø±Ø© ÙˆÙ…Ù†Ù‡Ø¬ÙŠØ© ÙˆØ§Ø¶Ø­Ø©.
- ÙŠØ¶ÙŠÙ Ø£Ù…Ø«Ù„Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© ÙˆØ­Ø³Ù‘ÙŠØ©.
- ÙŠÙ‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ù…Ø¯Ø§Ø±Ø³ Ø§Ù„ØªÙØ³ÙŠØ± (Ø§Ø¨Ù† Ø³ÙŠØ±ÙŠÙ†ØŒ Ø§Ù„Ù†Ø§Ø¨Ù„Ø³ÙŠØŒ Ø§Ø¨Ù† Ø´Ø§Ù‡ÙŠÙ†...).
- ÙŠØ¨Ø±Ø² Ø§Ù„Ø´ÙØ§ÙÙŠØ© (Ù‡Ø°Ø§ Ø§Ø¬ØªÙ‡Ø§Ø¯ Ù„Ø§ Ø­ØªÙ…ÙŠ).
- ÙŠÙ‚Ø¯Ù… Ù†ØµØ§Ø¦Ø­ Ø¹Ù…Ù„ÙŠØ© Ù„Ù„Ø±Ø§Ø¦ÙŠ.
- ÙŠØªÙØ§Ø¯Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø± ÙˆØ§Ù„Ø­Ø´Ùˆ.
- Ù…Ù†Ø¸Ù… Ø¨Ø¹Ù†Ø§ÙˆÙŠÙ† ÙØ±Ø¹ÙŠØ© ÙˆØ§Ø¶Ø­Ø©.
- ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙÙ‚Ø±Ø© Ø®ØªØ§Ù…ÙŠØ© ØªØ­ÙÙŠØ²ÙŠØ©.

--- Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ ---
{article}

--- Ø§Ù„Ù…Ù†Ø§ÙØ³ÙˆÙ† (Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ù…Ù† Ù†Ù‚Ø§Ø· Ù‚ÙˆØªÙ‡Ù…) ---
{competitor_texts}
"""
    response = openai.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.6,
    )
    return response.choices[0].message.content

def apply_internal_links(article, links_df):
    new_article = article
    for _, row in links_df.iterrows():
        word, link = row["Ø§Ù„ÙƒÙ„Ù…Ø©"], row["Ø§Ù„Ø±Ø§Ø¨Ø·"]
        if word in new_article:
            new_article = new_article.replace(word, f"[{word}]({link})", 1)
    return new_article

def export_docx(text, filename="article.docx"):
    doc = Document()
    doc.add_paragraph(text)
    doc.save(filename)
    return filename

# --------------- ÙˆØ§Ø¬Ù‡Ø© Streamlit -----------------
st.title("ğŸŒ™ Ù†Ø¸Ø§Ù… ØªØ­Ø³ÙŠÙ† Ù…Ù‚Ø§Ù„Ø§Øª ØªÙØ³ÙŠØ± Ø§Ù„Ø£Ø­Ù„Ø§Ù…")

with st.expander("ğŸ“¥ Ø£Ø¯Ø®Ù„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ù‚Ø§Ù„"):
    article_input = st.text_area("Ø§Ù„ØµÙ‚ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§:", height=300)

    col1, col2, col3 = st.columns(3)
    with col1:
        comp1 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 1")
    with col2:
        comp2 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 2")
    with col3:
        comp3 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 3")

competitors_texts = []
if comp1: competitors_texts.append(fetch_competitor_text(comp1))
if comp2: competitors_texts.append(fetch_competitor_text(comp2))
if comp3: competitors_texts.append(fetch_competitor_text(comp3))

if st.button("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†"):
    if article_input and competitors_texts:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„..."):
            report = analyze_article(article_input, competitors_texts)
        st.subheader("ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„")
        st.write(report)
    else:
        st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†.")

if st.button("âœï¸ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ù‚Ø§Ù„"):
    if article_input:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©..."):
            rewritten = rewrite_article(article_input, competitors_texts)
        st.subheader("ğŸ“ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†")
        st.session_state["rewritten_article"] = rewritten
        st.write(rewritten)
    else:
        st.warning("Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø£ÙˆÙ„Ø§Ù‹.")

if "rewritten_article" in st.session_state:
    st.subheader("ğŸ”— Ø£Ø¶Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©")
    links_df = st.data_editor(
        pd.DataFrame([{"Ø§Ù„ÙƒÙ„Ù…Ø©": "", "Ø§Ù„Ø±Ø§Ø¨Ø·": ""}]),
        num_rows="dynamic",
        use_container_width=True
    )
    if st.button("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©"):
        updated_article = apply_internal_links(st.session_state["rewritten_article"], links_df)
        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©")
        st.write(updated_article)
        st.session_state["final_article"] = updated_article

if "final_article" in st.session_state:
    st.subheader("ğŸ’¾ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„")
    final_text = st.session_state["final_article"]
    st.download_button("ØªØ­Ù…ÙŠÙ„ DOCX", data=final_text.encode("utf-8"), file_name="article.docx")
    st.download_button("ØªØ­Ù…ÙŠÙ„ Markdown", data=final_text.encode("utf-8"), file_name="article.md")
    json_data = json.dumps({"article": final_text}, ensure_ascii=False, indent=2)
    st.download_button("ØªØ­Ù…ÙŠÙ„ JSON", data=json_data.encode("utf-8"), file_name="article.json")
