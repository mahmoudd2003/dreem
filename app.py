# -*- coding: utf-8 -*-
import streamlit as st
import requests
from bs4 import BeautifulSoup
import openai
import pandas as pd
from docx import Document
import json, re, math

st.set_page_config(page_title="Dream Article Diagnostic & SEO Enhancer", page_icon="ğŸŒ™", layout="wide")

# ===== CONFIG =====
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY", "")
if not OPENAI_API_KEY:
    st.warning("âš ï¸ Ø£Ø¶Ù OPENAI_API_KEY ÙÙŠ Secrets Ù‚Ø¨Ù„ Ø§Ù„ØªØ´ØºÙŠÙ„.")
openai.api_key = OPENAI_API_KEY

@st.cache_data
def load_text(path):
    return open(path, "r", encoding="utf-8").read()

def load_json(path):
    return json.loads(load_text(path))

# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ ÙˆØ§Ù„Ù‚ÙˆØ§Ù„Ø¨
RULES = load_json("rules.json")
PROMPTS = load_json("prompts.json")

# ===== Helpers =====
def chat(model, messages, temperature=0.4):
    resp = openai.chat.completions.create(model=model, messages=messages, temperature=temperature)
    return resp.choices[0].message.content

def fetch_competitor_text(url):
    try:
        r = requests.get(url, timeout=12)
        r.raise_for_status()
        soup = BeautifulSoup(r.text, "html.parser")
        paragraphs = [p.get_text(" ", strip=True) for p in soup.find_all("p")]
        core = "\n".join([p for p in paragraphs if len(p.split()) > 4])
        return core[:15000]
    except Exception as e:
        return f"Ø®Ø·Ø£ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„Ù†Øµ: {e}"

def count_words(txt):
    return len(re.findall(r"\w+", txt, flags=re.UNICODE))

def keyword_density(text, keyword):
    if not keyword:
        return 0.0, 0, count_words(text)
    wc = count_words(text)
    k_count = len(re.findall(re.escape(keyword), text, flags=re.IGNORECASE))
    density = (k_count / max(wc,1)) * 100.0
    return round(density,2), k_count, wc

def rule_engine_precheck(text, focus, lsi_list, length_choice):
    res = {"checks": {}, "metrics": {}}
    wc = count_words(text)
    target = RULES["length_map"][length_choice]
    lo, hi = math.floor(target*0.9), math.ceil(target*1.1)
    res["metrics"]["word_count"] = wc
    res["metrics"]["target_range"] = [lo, hi]
    res["checks"]["length_ok"] = (lo <= wc <= hi)

    dens, kcount, _ = keyword_density(text, focus)
    res["metrics"]["focus_density_pct"] = dens
    res["metrics"]["focus_count"] = kcount
    lo_d, hi_d = RULES["focus_density_min_pct"], RULES["focus_density_max_pct"]
    res["checks"]["focus_density_ok"] = (dens >= lo_d and dens <= hi_d)

    lsi_report, lsi_ok = {}, True
    for k in lsi_list:
        c = len(re.findall(re.escape(k), text, flags=re.IGNORECASE))
        lsi_report[k] = c
        if c > RULES["lsi_max_occurrence"]:
            lsi_ok = False
    res["metrics"]["lsi_counts"] = lsi_report
    res["checks"]["lsi_ok"] = lsi_ok

    banned_hits = []
    for pat in RULES["banned_regex"]:
        if re.search(pat, text):
            banned_hits.append(pat)
    res["metrics"]["banned_hits"] = banned_hits
    res["checks"]["no_banned"] = (len(banned_hits) == 0)

    return res

def safe_json_loads(s):
    try:
        return json.loads(s)
    except Exception:
        m = re.search(r"(\{.*\})", s, flags=re.DOTALL)
        if m:
            try: 
                return json.loads(m.group(1))
            except Exception: 
                return {"raw": s, "error": "failed_json_parse"}
        return {"raw": s, "error": "no_json_found"}

# --- Safe placeholder replacement (avoids .format conflicts with JSON braces) ---
def fill(template: str, mapping: dict) -> str:
    out = template
    for k, v in mapping.items():
        out = out.replace("{"+k+"}", v)
    return out

# ===== LLM layers =====
def llm_diagnostic(article, competitors, focus, lsi_list):
    comp_block = "\n\n".join([f"[Ø§Ù„Ù…Ù†Ø§ÙØ³ {i+1}]\n{c}" for i, c in enumerate(competitors)])
    prompt = fill(PROMPTS["diagnostic"], {
        "ARTICLE": article,
        "COMPETITORS": comp_block,
        "FOCUS": focus,
        "LSI": ", ".join(lsi_list)
    })
    out = chat(PROMPTS["model"], [{"role":"user","content":prompt}], temperature=0.2)
    return safe_json_loads(out)

def llm_rewrite(article, competitors, focus, lsi_list, length_choice):
    comp_block = "\n\n".join([f"[Ø§Ù„Ù…Ù†Ø§ÙØ³ {i+1}]\n{c}" for i, c in enumerate(competitors)])
    length_desc = {
        "Ù‚ØµÙŠØ± (700-900 ÙƒÙ„Ù…Ø©)": "Ø­ÙˆØ§Ù„ÙŠ 800 ÙƒÙ„Ù…Ø©",
        "Ù…ØªÙˆØ³Ø· (1000-1300 ÙƒÙ„Ù…Ø©)": "Ø­ÙˆØ§Ù„ÙŠ 1200 ÙƒÙ„Ù…Ø©",
        "Ø·ÙˆÙŠÙ„ (1500-2000 ÙƒÙ„Ù…Ø©)": "Ø­ÙˆØ§Ù„ÙŠ 1700 ÙƒÙ„Ù…Ø©"
    }[length_choice]
    prompt = fill(PROMPTS["rewriter"], {
        "ARTICLE": article,
        "COMPETITORS": comp_block,
        "FOCUS": focus,
        "LSI": ", ".join(lsi_list),
        "LENGTH_DESC": length_desc
    })
    return chat(PROMPTS["model"], [{"role":"user","content":prompt}], temperature=0.6)

def llm_apply_fixes(text, fix_plan_json):
    # Fallback Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø§Ù„Ù…ÙØªØ§Ø­ "fixer" Ø¯Ø§Ø®Ù„ prompts.json
    fixer_tmpl = PROMPTS.get("fixer") or (
        "Ø³ÙŠØªÙ… ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ù†Øµ Ù…Ù‚Ø§Ù„ ÙˆØ¨Ø¹Ø¶ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (Fix Plan). "
        "Ø·Ø¨Ù‘Ù‚ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª ÙÙ‚Ø· Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„Ù„Ø§Ø²Ù…Ø© ÙˆØ£Ø¹Ø¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø¯Ù‘Ù„ ÙƒØ§Ù…Ù„Ø§Ù‹."
        "\n\n--- Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ø§Ù„ÙŠ ---\n{TEXT}\n\n--- Ø®Ø·Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­ ---\n{FIX_PLAN_JSON}\n"
    )
    prompt = fill(fixer_tmpl, {
        "TEXT": text,
        "FIX_PLAN_JSON": json.dumps(fix_plan_json, ensure_ascii=False, indent=2)
    })
    return chat(PROMPTS["model"], [{"role":"user","content":prompt}], temperature=0.5)

def llm_meta_faq(focus):
    # Fallback prompt Ø¥Ù† Ù„Ù… ÙŠÙˆØ¬Ø¯ "meta_faq" Ø£Ùˆ ÙƒØ§Ù† Ù„Ø§ ÙŠÙØ±Ø¬Ø¹ Ù…ØµØ§Ø¯Ø±
    meta_tmpl = PROMPTS.get("meta_faq") or (
        'Ø£Ù†Ø´Ø¦ JSON ÙŠØ­ÙˆÙŠ Title (â‰¤65) Ùˆ Description (â‰¤160) Ùˆ4 Ø£Ø³Ø¦Ù„Ø© FAQ Ù‚ØµÙŠØ±Ø© ÙˆØ¥Ø¬Ø§Ø¨Ø§ØªÙ‡Ø§ØŒ '
        'ÙˆØ£ÙŠØ¶Ø§Ù‹ Ù…ØµÙÙˆÙØ© "sources" (3-5 Ù…ØµØ§Ø¯Ø± Ø¨Ø£Ø³Ù…Ø§Ø¡ ÙˆØ±ÙˆØ§Ø¨Ø· Ø¹Ù†Ø¯Ù…Ø§ ØªØªÙˆÙØ±) Ù„Ù„ÙƒÙ„Ù…Ø© "{FOCUS}": '
        '{ "title":"...", "description":"...", "faq":[{"q":"...","a":"..."}], "sources":["...","..."] }'
    )
    prompt = fill(meta_tmpl, {"FOCUS": focus})
    out = chat(PROMPTS["model"], [{"role":"user","content":prompt}], temperature=0.4)
    data = safe_json_loads(out)
    return data

def llm_sources_from_scratch(focus):
    # Ù…ÙˆÙ„Ù‘Ø¯ Ù…ØµØ§Ø¯Ø± Ø¨Ø¯ÙŠÙ„ ÙŠØ·Ù„Ø¨ Ø£Ø³Ù…Ø§Ø¡ ÙˆØ§Ø¶Ø­Ø© ÙˆØ±ÙˆØ§Ø¨Ø· Ù…ÙˆØ«ÙˆÙ‚Ø© Ø¥Ù† ÙˆØ¬Ø¯Øª
    prompt = (
        "Ø§Ù‚ØªØ±Ø­ 3-5 Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø© Ù„Ù…Ù‚Ø§Ù„ Ø¹Ù† '{FOCUS}' Ø¨ØµÙŠØºØ© JSON: "
        '{ "sources": ["Ø§Ø³Ù… Ø§Ù„Ù…ØµØ¯Ø± - Ø±Ø§Ø¨Ø· Ø£Ùˆ Ù…Ù„Ø§Ø­Ø¸Ø© Ø§Ù„ÙˆØµÙˆÙ„", "..."] }'
    ).replace("{FOCUS}", focus)
    out = chat(PROMPTS["model"], [{"role":"user","content":prompt}], temperature=0.3)
    return safe_json_loads(out)

def apply_anchors(article, anchors_df):
    new_text = article
    applied, skipped = [], []
    for _, row in anchors_df.iterrows():
        phrase, link = (row.get("Ø§Ù„Ù†Øµ") or "").strip(), (row.get("Ø§Ù„Ø±Ø§Ø¨Ø·") or "").strip()
        if not phrase or not link: 
            continue
        if phrase in new_text:
            new_text = new_text.replace(phrase, f"[{phrase}]({link})", 1)
            applied.append(phrase)
        else:
            words = phrase.split()
            window = " ".join(words[:6]) if len(words) >= 6 else phrase
            if window in new_text:
                new_text = new_text.replace(window, f"[{window}]({link})", 1)
                applied.append(window + " (Ø¬Ø²Ø¦ÙŠ)")
            else:
                skipped.append(phrase)
    return new_text, applied, skipped

def export_docx(text):
    doc = Document()
    for line in text.split("\n"):
        doc.add_paragraph(line)
    doc.save("article.docx")
    with open("article.docx", "rb") as f:
        return f.read()

# ===== UI =====
st.title("ğŸŒ™ Dream Article Diagnostic & SEO Enhancer (LLM-Guided)")

with st.expander("ğŸ“¥ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„Ù…Ù†Ø§ÙØ³ÙŠÙ†", expanded=True):
    article_input = st.text_area("Ø§Ù„ØµÙ‚ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù‡Ù†Ø§:", height=260, key="article_input")
    focus_kw = st.text_input("ğŸ”‘ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© (Focus Keyword)", key="focus_kw")
    lsi_raw = st.text_area("ğŸ“Œ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© (Ø§ÙØµÙ„ Ø¨ÙŠÙ†Ù‡Ø§ Ø¨ÙØ§ØµÙ„Ø©)", key="lsi_raw")
    lsi_list = [k.strip() for k in lsi_raw.split(",") if k.strip()]
    length_choice = st.selectbox("ğŸ“ Ø·ÙˆÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨", ["Ù‚ØµÙŠØ± (700-900 ÙƒÙ„Ù…Ø©)","Ù…ØªÙˆØ³Ø· (1000-1300 ÙƒÙ„Ù…Ø©)","Ø·ÙˆÙŠÙ„ (1500-2000 ÙƒÙ„Ù…Ø©)"])

    c1,c2,c3 = st.columns(3)
    with c1: comp1 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 1", key="comp1")
    with c2: comp2 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 2", key="comp2")
    with c3: comp3 = st.text_input("Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø§ÙØ³ 3", key="comp3")

competitors_texts = []
for u in [st.session_state.get("comp1"), st.session_state.get("comp2"), st.session_state.get("comp3")]:
    if u: competitors_texts.append(fetch_competitor_text(u))

colA, colB = st.columns(2)
with colA:
    if st.button("ğŸ” ØªØ´Ø®ÙŠØµ Ù…Ø±Ø´Ø¯ (LLM Diagnostic)"):
        if not article_input or not focus_kw:
            st.warning("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹.")
        else:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ´Ø®ÙŠØµ..."):
                diag = llm_diagnostic(article_input, competitors_texts, focus_kw, lsi_list)
            st.session_state["diagnostic"] = diag
            st.subheader("ğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ´Ø®ÙŠØµ")
            st.json(diag)

with colB:
    if st.button("âœï¸ Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ù…Ø­Ø³Ù‘Ù†Ø©"):
        if not article_input or not focus_kw:
            st.warning("Ø£Ø¯Ø®Ù„ Ø§Ù„Ù…Ù‚Ø§Ù„ ÙˆØ§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹.")
        else:
            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø©..."):
                rewritten = llm_rewrite(article_input, competitors_texts, focus_kw, lsi_list, length_choice)
            st.session_state["rewritten"] = rewritten
            st.subheader("ğŸ“ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ† (Ù…Ø³ÙˆØ¯Ø© Ø£ÙˆÙ„Ù‰)")
            st.write(rewritten)

# Pre-check & Post-fix
if "rewritten" in st.session_state:
    st.markdown("---")
    st.subheader("ğŸ§ª ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø¬ÙˆØ¯Ø© (Rule Engine)")
    pre = rule_engine_precheck(st.session_state["rewritten"], focus_kw, lsi_list, length_choice)
    st.json(pre)

    if st.button("ğŸ§° ØªØ·Ø¨ÙŠÙ‚ Ø®Ø·Ø© Ø§Ù„Ø¥ØµÙ„Ø§Ø­ (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)"):
        diag = st.session_state.get("diagnostic", {})
        fix_plan = {"fixes": diag.get("fixes", [])} if isinstance(diag, dict) else {"fixes": []}
        if not fix_plan["fixes"]:
            st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Fixes Ù…Ù† Ø§Ù„ØªØ´Ø®ÙŠØµ. Ø³ÙŠØªÙ… Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ ÙƒÙ…Ø§ Ù‡Ùˆ.")
            st.session_state["final_text"] = st.session_state["rewritten"]
        else:
            with st.spinner("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©..."):
                fixed = llm_apply_fixes(st.session_state["rewritten"], fix_plan)
            st.session_state["final_text"] = fixed
        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¨Ø¹Ø¯ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª")
        st.write(st.session_state["final_text"])

# Meta / FAQ / Sources
if st.button("ğŸ§· ØªÙˆÙ„ÙŠØ¯ Meta & FAQ & Ù…ØµØ§Ø¯Ø±"):
    if not focus_kw:
        st.warning("Ø£Ø¯Ø®Ù„ Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹.")
    else:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯..."):
            meta = llm_meta_faq(focus_kw)
    # Ø¥Ù† Ù„Ù… ØªØªÙˆÙØ± Ù…ØµØ§Ø¯Ø± Ø¯Ø§Ø®Ù„ meta_faqØŒ Ø¬Ø±Ù‘Ø¨ Ø§Ù„ØªØ´Ø®ÙŠØµ Ø£Ùˆ ÙˆÙ„Ù‘Ø¯ Ù…Ù† Ø§Ù„ØµÙØ±:
    sources = []
    if isinstance(meta, dict):
        sources = meta.get("sources", []) or []
    if not sources:
        diag = st.session_state.get("diagnostic", {})
        if isinstance(diag, dict):
            meta_diag = diag.get("meta", {})
            sources = meta_diag.get("sources", []) or []
    if not sources:
        with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù‚ØªØ±Ø§Ø­ Ù…ØµØ§Ø¯Ø± Ù…ÙˆØ«ÙˆÙ‚Ø©..."):
            srcs = llm_sources_from_scratch(focus_kw)
            if isinstance(srcs, dict):
                sources = srcs.get("sources", []) or []

    # Ø¹Ø±Ø¶ Ø§Ù„Ù†Ø§ØªØ¬
    st.session_state["meta_block"] = {"title": meta.get("title") if isinstance(meta, dict) else None,
                                      "description": meta.get("description") if isinstance(meta, dict) else None,
                                      "faq": meta.get("faq") if isinstance(meta, dict) else [],
                                      "sources": sources}
    st.subheader("ğŸ“Œ SEO Outputs")
    st.json(st.session_state["meta_block"])

# Anchors
if "final_text" in st.session_state or "rewritten" in st.session_state:
    st.markdown("---")
    st.subheader("ğŸ”— Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ© (Anchors Ø¹Ù„Ù‰ Ø¬ÙÙ…Ù„)")
    anchors_df = st.data_editor(pd.DataFrame([{"Ø§Ù„Ù†Øµ":"", "Ø§Ù„Ø±Ø§Ø¨Ø·":""}]), num_rows="dynamic", use_container_width=True, key="anchors_df")
    if st.button("ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©"):
        base_text = st.session_state.get("final_text") or st.session_state.get("rewritten", "")
        updated, applied, skipped = apply_anchors(base_text, anchors_df)
        st.session_state["final_with_anchors"] = updated
        st.success(f"ØªÙ… ØªØ·Ø¨ÙŠÙ‚ {len(applied)} Ø±Ø§Ø¨Ø·. Ø§Ù„Ù…ØªØ¹Ø°Ø±: {len(skipped)}")
        if skipped: st.write("ØªØ¹Ø°Ø± ØªØ·Ø¨ÙŠÙ‚ Ø¹Ù„Ù‰:", skipped)
        st.subheader("ğŸ“„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù…Ø¹ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©")
        st.write(updated)

# Downloads
final_output = st.session_state.get("final_with_anchors") or st.session_state.get("final_text") or st.session_state.get("rewritten")
if final_output:
    st.markdown("---")
    st.subheader("ğŸ’¾ ØªÙ†Ø²ÙŠÙ„")
    md_bytes = final_output.encode("utf-8")
    st.download_button("ØªØ­Ù…ÙŠÙ„ Markdown", data=md_bytes, file_name="article.md")
    json_bytes = json.dumps({"article": final_output}, ensure_ascii=False, indent=2).encode("utf-8")
    st.download_button("ØªØ­Ù…ÙŠÙ„ JSON", data=json_bytes, file_name="article.json")
    docx_bytes = export_docx(final_output)
    st.download_button("ØªØ­Ù…ÙŠÙ„ DOCX", data=docx_bytes, file_name="article.docx")
